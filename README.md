# ğŸ¥ Real-Time Healthcare Data Platform
### Patient Flow & Bed Occupancy Analytics â€” End-to-End Azure Data Engineering

<p align="center">
  <img src="docs/architecture_diagram.png" alt="Architecture Diagram" width="100%"/>
</p>

<p align="center">
  <a href="#architecture"><img src="https://img.shields.io/badge/Azure-Databricks-FF3621?style=flat-square&logo=databricks&logoColor=white" /></a>
  <a href="#architecture"><img src="https://img.shields.io/badge/Azure-Data_Factory-0078D4?style=flat-square&logo=microsoftazure&logoColor=white" /></a>
  <a href="#architecture"><img src="https://img.shields.io/badge/Azure-Synapse-0078D4?style=flat-square&logo=microsoftazure&logoColor=white" /></a>
  <a href="#architecture"><img src="https://img.shields.io/badge/Azure-Event_Hubs-0078D4?style=flat-square&logo=microsoftazure&logoColor=white" /></a>
  <a href="#architecture"><img src="https://img.shields.io/badge/Delta-Lake-003366?style=flat-square&logo=delta&logoColor=white" /></a>
  <a href="#architecture"><img src="https://img.shields.io/badge/Python-3.11-3776AB?style=flat-square&logo=python&logoColor=white" /></a>
</p>

---

## ğŸ“‹ Project Overview

A production-grade, end-to-end data engineering platform built for **Midwest Health Alliance (MHA)**, a fictional network of 7 hospitals. The platform ingests real-time patient admission and discharge events, processes them through a Medallion Architecture (Bronze â†’ Silver â†’ Gold), and serves analytics-ready data through Azure Synapse for dashboarding.

**Business Problem:** MHA lacked a centralized, real-time system to monitor bed occupancy, patient flow patterns, and department-level bottlenecks â€” especially critical during seasonal surges like flu outbreaks.

**Solution:** An automated Azure-based pipeline that streams patient events, cleans and validates data, builds a Star Schema data warehouse, and delivers KPIs including occupancy rates, average length of stay, gender-based demographics, and clinical vitals by department.

---

## ğŸ—ï¸ Architecture

<a name="architecture"></a>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Patient    â”‚     â”‚    Azure     â”‚     â”‚          Azure Databricks + ADLS Gen2        â”‚     â”‚    Azure      â”‚
â”‚    Flow      â”‚â”€â”€â”€â”€â–¶â”‚  Event Hubs  â”‚â”€â”€â”€â”€â–¶â”‚                                              â”‚â”€â”€â”€â”€â–¶â”‚   Synapse     â”‚
â”‚  Simulator   â”‚     â”‚  (Kafka)     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  Serverless   â”‚
â”‚  (Python)    â”‚     â”‚              â”‚     â”‚  â”‚ BRONZE  â”‚â”€â–¶â”‚ SILVER  â”‚â”€â–¶â”‚    GOLD     â”‚  â”‚     â”‚  SQL Pool     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚ Raw JSONâ”‚  â”‚Cleaned +â”‚  â”‚Star Schema â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚  â”‚         â”‚  â”‚  DQ Flagsâ”‚  â”‚ SCD Type 2 â”‚  â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
       â”‚  Azure Key     â”‚                 â”‚                                              â”‚
       â”‚  Vault         â”‚â”€â”€â”€ Secrets â”€â”€â”€â”€â–¶â”‚     Orchestrated by Azure Data Factory       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Breakdown

| Component | Service | Purpose |
|-----------|---------|---------|
| **Data Source** | Python Simulator | Generates realistic patient admission/discharge events with configurable dirty data |
| **Streaming** | Azure Event Hubs (Kafka) | Ingests real-time events via Kafka protocol on port 9093 |
| **Orchestration** | Azure Data Factory | Chains Bronze â†’ Silver â†’ Gold notebooks with success/failure triggers |
| **Processing** | Azure Databricks (PySpark) | Runs Medallion Architecture transformations across 3 notebook layers |
| **Storage** | ADLS Gen2 (Delta Lake) | Stores all layers as Delta tables with schema evolution support |
| **Security** | Azure Key Vault | Stores Event Hub connection strings and storage account keys |
| **Serving** | Azure Synapse (Serverless SQL) | Exposes Gold layer via external tables and views for analytics |

---

## ğŸ“‚ Project Structure

```
healthcare-analytics/
â”œâ”€â”€ simulator/
â”‚   â”œâ”€â”€ patient_flow_simulator.py    # Event generator with 7 dirty data scenarios
â”‚   â”œâ”€â”€ .env.example                 # Environment variable template
â”‚   â””â”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py       # Event Hub â†’ Bronze (raw JSON + metadata)
â”‚   â”œâ”€â”€ 02_silver_data_cleaning.py   # Bronze â†’ Silver (7 DQ rules + deduplication)
â”‚   â””â”€â”€ 03_gold_data_transform.py    # Silver â†’ Gold (Star Schema + SCD2)
â”‚
â”œâ”€â”€ synapse/
â”‚   â”œâ”€â”€ synapse_external_tables.sql  # External tables pointing to Gold Delta files
â”‚   â””â”€â”€ synapse_fix_views.sql        # Deduplicated views for analytics
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png     # System architecture visual
â”‚   â””â”€â”€ pipeline_architecture.html   # Interactive architecture reference
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ”§ Technical Deep Dive

### 1. Patient Flow Simulator (`simulator/`)

A Python-based event generator that simulates realistic hospital operations across 7 hospitals and 7 departments.

**Key Features:**
- **Admission â†’ Discharge Lifecycle:** Tracks active patients in memory; generates separate `ADMISSION` and `DISCHARGE` events for the same `patient_id`, simulating real hospital data flow
- **Rich Data Model:** 19+ fields per event including ICD-10 diagnosis codes, insurance type, admission priority (weighted distribution), vitals (heart rate, BP, O2 saturation, temperature), and structured bed IDs (`H3-SUR-15`)
- **7 Configurable Dirty Data Scenarios:**
  - Invalid age (negative, >120)
  - Future admission timestamps
  - Missing/null critical fields
  - Discharge before admission
  - Department name typos (`Emergancy`, `PEDS`, `Cardiolgy`)
  - Invalid hospital IDs (outside 1-7)
  - Duplicate events (same event sent twice)
- **Seasonal Surge Simulation:** 3x admission rates during flu season (Dec-Feb) and weekend evenings
- **CLI Interface:** Supports `--dry-run`, `--max-events`, and `--connection-string` flags
- **Environment Variable Security:** Connection strings loaded from environment, never hardcoded

```bash
# Dry run (no Event Hub needed)
python patient_flow_simulator.py --dry-run --max-events 20

# Live streaming
export EVENTHUB_CONNECTION_STRING="Endpoint=sb://..."
python patient_flow_simulator.py --max-events 500
```

### 2. Bronze Layer â€” Raw Ingestion (`01_bronze_ingestion.py`)

Reads from Azure Event Hubs via Spark Structured Streaming (Kafka protocol) and lands raw JSON into Delta format.

**What it captures:**
- Raw JSON payload (untouched)
- Kafka metadata: `topic`, `partition`, `offset`, `eventhub_timestamp`
- Ingestion metadata: `ingested_at`, `data_source`
- Schema evolution enabled (`mergeSchema: true`)

**Design Decisions:**
- Credentials fetched from Databricks Secret Scope (backed by Azure Key Vault)
- `maxOffsetsPerTrigger: 1000` to control micro-batch size for cost management
- 30-second trigger interval balancing near-real-time with compute costs

### 3. Silver Layer â€” Data Cleaning (`02_silver_data_cleaning.py`)

Parses raw JSON against the expected schema and applies data quality rules for all 7 dirty data scenarios.

**Data Quality Rules Applied:**

| Rule | Detection | Action | DQ Flag |
|------|-----------|--------|---------|
| Future admission time | `admission_time > current_timestamp()` | Replace with `event_timestamp` | `FUTURE_ADMISSION` |
| Invalid age | `age < 0 OR age > 120` | Set to `NULL` | `INVALID_AGE` |
| Discharge before admission | `discharge_time < admission_time` | Null out discharge | `DISCHARGE_BEFORE_ADMIT` |
| Department typo | Not in valid list | Map to correct name | `DEPT_TYPO` |
| Invalid hospital ID | `< 1 OR > 7` | Set to `NULL` | `INVALID_HOSPITAL_ID` |
| Missing critical fields | `patient_id`, `event_type`, or `department` is null | Flag only | `MISSING_{FIELD}` |
| Duplicate events | Same `event_id` | `dropDuplicates()` | â€” |

**Key Feature:** Every record gets a `_dq_flags` column (e.g., `FUTURE_ADMISSION\|DEPT_TYPO`) providing full audit trail of what was corrected â€” not just silently fixed.

### 4. Gold Layer â€” Star Schema (`03_gold_data_transform.py`)

Transforms Silver data into a dimensional model optimized for analytics.

**Star Schema:**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_patient   â”‚
                    â”‚   (SCD Type 2)  â”‚
                    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                    â”‚ patient_sk (PK) â”‚
                    â”‚ patient_id      â”‚
                    â”‚ gender          â”‚
                    â”‚ age             â”‚
                    â”‚ insurance_type  â”‚
                    â”‚ effective_from  â”‚
                    â”‚ effective_to    â”‚
                    â”‚ is_current      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_department  â”‚   â”‚    fact_patient_flow         â”‚   â”‚    dim_date      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ department_sk   â”‚â—„â”€â”€â”‚ fact_id (PK)                â”‚â”€â”€â–¶â”‚ date_key (PK)    â”‚
â”‚ department      â”‚   â”‚ patient_sk (FK)             â”‚   â”‚ year             â”‚
â”‚ hospital_id     â”‚   â”‚ department_sk (FK)          â”‚   â”‚ quarter          â”‚
â”‚ hospital_name   â”‚   â”‚ admission_date (FK)         â”‚   â”‚ month / month_nameâ”‚
â”‚ bed_capacity    â”‚   â”‚ event_type                  â”‚   â”‚ day / day_name   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ admission_time              â”‚   â”‚ day_of_week      â”‚
                      â”‚ discharge_time              â”‚   â”‚ is_weekend       â”‚
                      â”‚ length_of_stay_hrs          â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ admission_priority          â”‚
                      â”‚ diagnosis_code              â”‚
                      â”‚ heart_rate, bp_systolic ... â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SCD Type 2 Implementation:** The `dim_patient` table tracks historical changes using hash-based change detection. When a patient's `gender`, `age`, or `insurance_type` changes between events, the old record is expired (`is_current = false`, `effective_to` set) and a new current record is inserted.

**Vitals Flattening:** The nested `vitals` struct from Silver is flattened into individual columns (`heart_rate`, `bp_systolic`, `bp_diastolic`, `temperature_f`, `oxygen_saturation`) for direct query access in Synapse and dashboards.

### 5. Synapse Analytics Layer (`synapse/`)

Azure Synapse Serverless SQL pool exposes Gold Delta tables via external tables and deduplicated views.

**Views handle:**
- Duplicate surrogate key resolution (Spark's `monotonically_increasing_id` can produce duplicates across partitions)
- `ROW_NUMBER()` reassignment for unique dimension keys
- Old-to-new key mapping for fact table foreign keys

**Sample KPIs Available:**
- Current occupancy % by department and hospital
- Gender-based occupancy distribution
- Average length of stay by department
- Admissions by priority type and day of week
- Top diagnoses by volume
- Average vitals (HR, BP, O2 sat) by department

### 6. ADF Orchestration

Azure Data Factory pipeline chains the 3 Databricks notebooks:

```
[01_bronze_ingestion] â”€â”€âœ…â”€â”€â–¶ [02_silver_data_cleaning] â”€â”€âœ…â”€â”€â–¶ [03_gold_data_transform]
                       â”‚                                  â”‚
                       âŒâ”€â”€â–¶ Alert                        âŒâ”€â”€â–¶ Alert
```

---

## ğŸš€ Setup & Deployment

### Prerequisites
- Azure account with active subscription
- Azure Databricks workspace
- Azure Event Hubs namespace
- ADLS Gen2 storage account with containers: `bronze`, `silver`, `gold`
- Azure Key Vault with secrets for Event Hub and Storage connections
- Python 3.11+

### Step 1: Configure Secrets

```bash
# Databricks CLI â€” create secret scope backed by Key Vault
databricks secrets create-scope --scope healthcare-analytics
databricks secrets put --scope healthcare-analytics --key eventhub-connection-string
databricks secrets put --scope healthcare-analytics --key adls-access-key
```

### Step 2: Run the Simulator

```bash
cd simulator/
pip install -r requirements.txt

# Set connection string
export EVENTHUB_CONNECTION_STRING="Endpoint=sb://your-namespace.servicebus.windows.net/;..."

# Stream events
python patient_flow_simulator.py --max-events 500
```

### Step 3: Run Databricks Notebooks

Import the 3 notebooks into your Databricks workspace and run in order:
1. `01_bronze_ingestion.py` â€” Streams Event Hub data to Bronze
2. `02_silver_data_cleaning.py` â€” Cleans and validates data
3. `03_gold_data_transform.py` â€” Builds Star Schema

### Step 4: Set Up Synapse

Run the SQL scripts in `synapse/` against your Synapse Serverless SQL pool to create external tables and views.

### Step 5: Configure ADF Pipeline

Create a pipeline with 3 Databricks Notebook activities chained with success dependencies.

---

## ğŸ“Š Key Metrics & Results

| Metric | Value |
|--------|-------|
| Events Processed | ~2,400+ patient events |
| Hospitals Covered | 7 |
| Departments | 7 (Emergency, Surgery, ICU, Pediatrics, Maternity, Oncology, Cardiology) |
| Dirty Data Scenarios | 7 types injected and handled |
| Data Quality Coverage | 100% of records flagged with `_dq_flags` audit trail |
| Schema Fields | 19+ per event (vs 8 in standard implementations) |
| Pipeline Automation | Fully orchestrated via ADF with failure alerting |

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Languages** | Python 3.11, PySpark, SQL |
| **Streaming** | Azure Event Hubs (Kafka protocol), Spark Structured Streaming |
| **Processing** | Azure Databricks, Delta Lake |
| **Storage** | Azure Data Lake Storage Gen2 |
| **Orchestration** | Azure Data Factory |
| **Data Warehouse** | Azure Synapse Analytics (Serverless SQL) |
| **Security** | Azure Key Vault, Databricks Secret Scopes, RBAC |
| **Data Modeling** | Star Schema, SCD Type 2, Medallion Architecture |
| **Version Control** | Git / GitHub |

---

## ğŸ§  What I Learned

- Designing and implementing a **Medallion Architecture** (Bronze/Silver/Gold) with Delta Lake for reliable, incremental data processing
- Building **SCD Type 2** dimensions with hash-based change detection for tracking historical attribute changes
- Connecting Azure Event Hubs via **Kafka protocol** for real-time streaming ingestion
- Implementing **7 data quality rules** with audit trail flags â€” not just fixing data, but documenting what was fixed
- Managing **schema evolution** gracefully so new fields don't break existing pipelines
- Using **Azure Key Vault + Databricks Secret Scopes** for production-grade secrets management
- Orchestrating multi-notebook pipelines with **Azure Data Factory** including failure handling
- Creating **Synapse Serverless SQL views** to resolve surrogate key issues without reprocessing data
- Cost optimization strategies for Azure free tier (batch vs. streaming, trigger intervals, single-node clusters)

---

## ğŸ“„ License

This project is for educational and portfolio purposes. The patient data is entirely simulated â€” no real patient information is used.

## Reference and Shoutout 

Huge thanks and shoutout to Jaya Chandra (https://github.com/Jay61616) for a comprehensive and in-dept demo of this project. 